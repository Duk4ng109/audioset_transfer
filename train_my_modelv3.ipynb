{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to do , I am using 2 different computational graphs, so need 2 different sessions\n",
    "#[ done ] \n",
    "\n",
    "# use rnn to condense features into a single feature vector\n",
    "#  [ pending ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saurabh/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import six\n",
    "#import tensorflow as tf\n",
    "\n",
    "import vggish_input\n",
    "import vggish_params\n",
    "import vggish_postprocess\n",
    "import vggish_slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_classes=4\n",
    "data_base_directory='data/datav2/'\n",
    "alphabet_dict={0:'A' , 1:'B' , 2:'C' , 3:'D' , 4:'E', 5:'F' , 6:'G' ,7 :'H' , 8:'I', 9:'J'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_function(input):\n",
    "  \n",
    "  \n",
    "\n",
    "  input=data_base_directory + str(input ,'utf-8')\n",
    "  #print(str(input,'utf-8'))\n",
    "\n",
    " \n",
    "\n",
    "  for i in range(num_classes):\n",
    "     if alphabet_dict[i] in input:\n",
    "         y=int(i)\n",
    "\n",
    "  #if 'A' in str(input,'utf-8'):\n",
    "   #     y=0\n",
    " \n",
    "\n",
    "\n",
    "  \n",
    "    \n",
    "  return  input,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[b'data/datav2/20_54_21011B.wav']\n",
      "[b'data/datav2/20_50_46073A.wav']\n",
      "[b'data/datav2/20_50_46041A.wav']\n",
      "[b'data/datav2/20_54_21006B.wav']\n",
      "[b'data/datav2/20_50_46033A.wav']\n",
      "[b'data/datav2/20_48_00007A.wav']\n",
      "[b'data/datav2/20_50_46099A.wav']\n",
      "[b'data/datav2/20_48_00022A.wav']\n",
      "[b'data/datav2/20_50_46106A.wav']\n",
      "[b'data/datav2/20_50_46025A.wav']\n",
      "[b'data/datav2/20_50_46118A.wav']\n",
      "[b'data/datav2/20_54_21002B.wav']\n",
      "[b'data/datav2/20_48_00019A.wav']\n",
      "[b'data/datav2/20_50_46027A.wav']\n",
      "[b'data/datav2/20_50_46059A.wav']\n",
      "[b'data/datav2/20_48_00008A.wav']\n",
      "[b'data/datav2/20_50_46095A.wav']\n",
      "[b'data/datav2/20_50_46101A.wav']\n",
      "[b'data/datav2/20_48_00009A.wav']\n",
      "[b'data/datav2/20_50_46064A.wav']\n",
      "[b'data/datav2/20_54_21041B.wav']\n",
      "[b'data/datav2/20_50_46070A.wav']\n",
      "[b'data/datav2/20_50_46058A.wav']\n",
      "[b'data/datav2/20_50_46100A.wav']\n",
      "[b'data/datav2/20_50_46086A.wav']\n",
      "[b'data/datav2/20_50_46038A.wav']\n",
      "[b'data/datav2/20_50_46060A.wav']\n",
      "[b'data/datav2/20_50_46091A.wav']\n",
      "[b'data/datav2/20_50_46076A.wav']\n",
      "[b'data/datav2/20_50_46011A.wav']\n",
      "[b'data/datav2/20_54_21035B.wav']\n",
      "[b'data/datav2/20_50_46021A.wav']\n",
      "[b'data/datav2/20_54_21030B.wav']\n",
      "[b'data/datav2/20_54_21021B.wav']\n",
      "[b'data/datav2/20_54_21005B.wav']\n",
      "[b'data/datav2/20_54_21050B.wav']\n",
      "[b'data/datav2/20_48_00002A.wav']\n",
      "[b'data/datav2/20_48_00025A.wav']\n",
      "[b'data/datav2/20_50_46093A.wav']\n",
      "[b'data/datav2/20_54_21048B.wav']\n",
      "[b'data/datav2/20_50_46078A.wav']\n",
      "[b'data/datav2/20_50_46094A.wav']\n",
      "[b'data/datav2/20_50_46048A.wav']\n",
      "[b'data/datav2/20_56_39024B.wav']\n",
      "[b'data/datav2/20_56_39001B.wav']\n",
      "[b'data/datav2/20_50_46129A.wav']\n",
      "[b'data/datav2/20_50_46087A.wav']\n",
      "[b'data/datav2/20_50_46049A.wav']\n",
      "[b'data/datav2/20_54_21046B.wav']\n",
      "[b'data/datav2/20_50_46013A.wav']\n",
      "[b'data/datav2/20_54_21027B.wav']\n",
      "[b'data/datav2/20_54_21017B.wav']\n",
      "[b'data/datav2/20_50_46124A.wav']\n",
      "[b'data/datav2/20_50_46012A.wav']\n",
      "[b'data/datav2/20_54_21004B.wav']\n",
      "[b'data/datav2/20_50_46122A.wav']\n",
      "[b'data/datav2/20_54_21044B.wav']\n",
      "[b'data/datav2/20_56_39039B.wav']\n",
      "[b'data/datav2/20_48_00005A.wav']\n",
      "[b'data/datav2/20_54_21007B.wav']\n",
      "[b'data/datav2/20_50_46056A.wav']\n",
      "[b'data/datav2/20_50_46018A.wav']\n",
      "[b'data/datav2/20_50_46051A.wav']\n",
      "[b'data/datav2/20_56_39042B.wav']\n",
      "[b'data/datav2/20_56_39041B.wav']\n",
      "[b'data/datav2/20_54_21012B.wav']\n",
      "[b'data/datav2/20_50_46029A.wav']\n",
      "[b'data/datav2/20_48_00010A.wav']\n",
      "[b'data/datav2/20_48_00004A.wav']\n",
      "[b'data/datav2/20_50_46047A.wav']\n",
      "[b'data/datav2/20_56_39038B.wav']\n",
      "[b'data/datav2/20_54_21049B.wav']\n",
      "[b'data/datav2/20_50_46054A.wav']\n",
      "[b'data/datav2/20_54_21040B.wav']\n",
      "[b'data/datav2/20_50_46126A.wav']\n",
      "[b'data/datav2/20_54_21024B.wav']\n",
      "[b'data/datav2/20_50_46084A.wav']\n",
      "[b'data/datav2/20_50_46092A.wav']\n",
      "[b'data/datav2/20_56_39025B.wav']\n",
      "[b'data/datav2/20_50_46090A.wav']\n",
      "[b'data/datav2/20_56_39016B.wav']\n",
      "[b'data/datav2/20_50_46057A.wav']\n",
      "[b'data/datav2/20_58_52001C.wav']\n",
      "[b'data/datav2/20_50_46028A.wav']\n",
      "[b'data/datav2/20_54_21008B.wav']\n",
      "[b'data/datav2/20_50_46017A.wav']\n",
      "[b'data/datav2/20_50_46030A.wav']\n",
      "[b'data/datav2/20_54_21015B.wav']\n",
      "[b'data/datav2/20_54_21020B.wav']\n",
      "[b'data/datav2/20_50_46109A.wav']\n",
      "[b'data/datav2/20_50_46002A.wav']\n",
      "[b'data/datav2/20_50_46107A.wav']\n",
      "[b'data/datav2/20_56_39010B.wav']\n",
      "[b'data/datav2/20_48_00011A.wav']\n",
      "[b'data/datav2/20_54_21010B.wav']\n",
      "[b'data/datav2/20_56_39032B.wav']\n",
      "[b'data/datav2/20_50_46022A.wav']\n",
      "[b'data/datav2/20_56_39003B.wav']\n",
      "[b'data/datav2/20_58_52021C.wav']\n",
      "[b'data/datav2/20_54_21028B.wav']\n",
      "[b'data/datav2/20_58_52040C.wav']\n",
      "[b'data/datav2/20_58_52034C.wav']\n",
      "[b'data/datav2/20_58_52000C.wav']\n",
      "[b'data/datav2/20_48_00023A.wav']\n",
      "[b'data/datav2/20_50_46130A.wav']\n",
      "[b'data/datav2/20_58_52006C.wav']\n",
      "[b'data/datav2/20_58_52026C.wav']\n",
      "[b'data/datav2/20_50_46026A.wav']\n",
      "[b'data/datav2/20_54_21013B.wav']\n",
      "[b'data/datav2/20_58_52044C.wav']\n",
      "[b'data/datav2/20_50_46062A.wav']\n",
      "[b'data/datav2/20_58_52010C.wav']\n",
      "[b'data/datav2/20_48_00024A.wav']\n",
      "[b'data/datav2/20_50_46074A.wav']\n",
      "[b'data/datav2/20_50_46098A.wav']\n",
      "[b'data/datav2/20_58_52009C.wav']\n",
      "[b'data/datav2/20_48_00027A.wav']\n",
      "[b'data/datav2/20_50_46127A.wav']\n",
      "[b'data/datav2/20_58_52007C.wav']\n",
      "[b'data/datav2/20_50_46065A.wav']\n",
      "[b'data/datav2/20_48_00026A.wav']\n",
      "[b'data/datav2/20_58_52037C.wav']\n",
      "[b'data/datav2/20_58_52050C.wav']\n",
      "[b'data/datav2/20_50_46007A.wav']\n",
      "[b'data/datav2/20_48_00001A.wav']\n",
      "[b'data/datav2/20_56_39002B.wav']\n",
      "[b'data/datav2/20_50_46125A.wav']\n",
      "[b'data/datav2/20_50_46072A.wav']\n",
      "[b'data/datav2/20_58_52035C.wav']\n",
      "[b'data/datav2/20_54_21023B.wav']\n",
      "[b'data/datav2/20_58_52028C.wav']\n",
      "[b'data/datav2/20_54_21043B.wav']\n",
      "[b'data/datav2/20_58_52018C.wav']\n",
      "[b'data/datav2/20_56_39036B.wav']\n",
      "[b'data/datav2/20_58_52074C.wav']\n",
      "[b'data/datav2/20_54_21019B.wav']\n",
      "[b'data/datav2/20_58_52076C.wav']\n",
      "[b'data/datav2/20_54_21018B.wav']\n",
      "[b'data/datav2/20_50_46068A.wav']\n",
      "[b'data/datav2/20_56_39014B.wav']\n",
      "[b'data/datav2/20_50_46085A.wav']\n",
      "[b'data/datav2/20_56_39006B.wav']\n",
      "[b'data/datav2/20_50_46108A.wav']\n",
      "[b'data/datav2/20_50_46119A.wav']\n",
      "[b'data/datav2/20_50_46019A.wav']\n",
      "[b'data/datav2/20_58_52083C.wav']\n",
      "[b'data/datav2/20_50_46131A.wav']\n",
      "[b'data/datav2/20_50_46037A.wav']\n",
      "[b'data/datav2/20_58_52031C.wav']\n",
      "[b'data/datav2/20_50_46080A.wav']\n",
      "[b'data/datav2/20_58_52024C.wav']\n",
      "[b'data/datav2/20_58_52025C.wav']\n",
      "[b'data/datav2/20_58_52020C.wav']\n",
      "[b'data/datav2/20_50_46000A.wav']\n",
      "[b'data/datav2/20_50_46096A.wav']\n",
      "[b'data/datav2/20_54_21031B.wav']\n",
      "[b'data/datav2/20_54_21016B.wav']\n",
      "[b'data/datav2/20_54_21029B.wav']\n",
      "[b'data/datav2/20_58_52070C.wav']\n",
      "[b'data/datav2/20_58_52008C.wav']\n",
      "[b'data/datav2/20_56_39012B.wav']\n",
      "[b'data/datav2/20_58_52022C.wav']\n",
      "[b'data/datav2/20_50_46003A.wav']\n",
      "[b'data/datav2/20_58_52085C.wav']\n",
      "[b'data/datav2/20_58_52087C.wav']\n",
      "[b'data/datav2/20_54_21025B.wav']\n",
      "[b'data/datav2/20_54_21036B.wav']\n",
      "[b'data/datav2/20_58_52067C.wav']\n",
      "[b'data/datav2/20_54_21001B.wav']\n",
      "[b'data/datav2/20_54_21038B.wav']\n",
      "[b'data/datav2/20_50_46061A.wav']\n",
      "[b'data/datav2/20_58_52015C.wav']\n",
      "[b'data/datav2/20_58_52049C.wav']\n",
      "[b'data/datav2/20_50_46005A.wav']\n",
      "[b'data/datav2/20_50_46112A.wav']\n",
      "[b'data/datav2/20_50_46113A.wav']\n",
      "[b'data/datav2/20_50_46102A.wav']\n",
      "[b'data/datav2/20_58_52086C.wav']\n",
      "[b'data/datav2/20_56_39004B.wav']\n",
      "[b'data/datav2/20_58_52111C.wav']\n",
      "[b'data/datav2/20_56_39026B.wav']\n",
      "[b'data/datav2/20_48_00020A.wav']\n",
      "[b'data/datav2/20_58_52116C.wav']\n",
      "[b'data/datav2/20_58_52107C.wav']\n",
      "[b'data/datav2/20_50_46055A.wav']\n",
      "[b'data/datav2/20_58_52033C.wav']\n",
      "[b'data/datav2/20_56_39022B.wav']\n",
      "[b'data/datav2/20_58_52062C.wav']\n",
      "[b'data/datav2/20_48_00018A.wav']\n",
      "[b'data/datav2/20_50_46067A.wav']\n",
      "[b'data/datav2/20_58_52063C.wav']\n",
      "[b'data/datav2/20_58_52120C.wav']\n",
      "[b'data/datav2/20_56_39008B.wav']\n",
      "[b'data/datav2/20_58_52118C.wav']\n",
      "[b'data/datav2/20_50_46036A.wav']\n",
      "[b'data/datav2/20_50_46063A.wav']\n",
      "[b'data/datav2/20_58_52117C.wav']\n",
      "[b'data/datav2/20_58_52079C.wav']\n",
      "[b'data/datav2/20_58_52096C.wav']\n",
      "[b'data/datav2/20_58_52004C.wav']\n"
     ]
    }
   ],
   "source": [
    "logfilepath='data/dataset.txt'\n",
    "dataset = tf.data.TextLineDataset(logfilepath).shuffle(buffer_size=200) #.map(map_function)\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda filename: tuple(tf.py_func(\n",
    "        process_function, [filename],  [tf.string,tf.int64])))\n",
    "\n",
    "dataset=dataset.batch(1)\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "iterator = dataset.make_initializable_iterator()\n",
    "next_element = iterator.get_next()\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(iterator.initializer)\n",
    "    for i in range(200):\n",
    "      value = sess.run(next_element[0])\n",
    "      print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.01\n",
    "num_steps = 500\n",
    "batch_size = 2\n",
    "display_step = 100\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 256 # 1st layer number of neurons\n",
    "n_hidden_2 = 256 # 2nd layer number of neurons\n",
    "num_input = 128 # MNIST data input (img shape: 28*28)\n",
    "#num_classes = 5 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, num_input])\n",
    "Y = tf.placeholder(tf.int32, [None])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([num_input, n_hidden_1])),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, num_classes]))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1])),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2])),\n",
    "    'out': tf.Variable(tf.random_normal([num_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def neural_net(x):\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_1 =  tf.nn.relu (tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "    # Hidden fully connected layer with 256 neurons\n",
    "    layer_2 =  tf.nn.relu(tf.add(tf.matmul(layer_1, weights['h2']), biases['b2']))\n",
    "    # Output fully connected layer with a neuron for each class\n",
    "    out_layer = tf.matmul(layer_2, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "Logits = neural_net(X)\n",
    "\n",
    "Logits = tf.Print(Logits, [Logits,tf.shape(Logits)], message=\"This is a: \")\n",
    "\n",
    "# Define loss and optimizer\n",
    "#loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    " #   logits=logits, labels=Y))\n",
    "prediction = tf.nn.softmax(Logits)\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=Logits))\n",
    "train_op = optimizer.minimize(cost)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(Logits, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from vggish_model.ckpt\n"
     ]
    }
   ],
   "source": [
    "g_1 = tf.Graph()\n",
    "with g_1.as_default():\n",
    "  \n",
    "\n",
    "                \n",
    "                \n",
    "  \n",
    "\n",
    "  # Sessions created in this scope will run operations from `g_1`.\n",
    "                sess1 = tf.Session()\n",
    "        \n",
    "                vggish_slim.define_vggish_slim(training=False)\n",
    "                vggish_slim.load_vggish_slim_checkpoint(sess1, 'vggish_model.ckpt')\n",
    "                features_tensor = sess1.graph.get_tensor_by_name(\n",
    "                vggish_params.INPUT_TENSOR_NAME)\n",
    "                embedding_tensor = sess1.graph.get_tensor_by_name(\n",
    "                vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "                sess.run(init)\n",
    "\n",
    "   \n",
    "                sess.run(iterator.initializer)\n",
    "\n",
    "               \n",
    "                \n",
    "                examples_batch = vggish_input.wavfile_to_examples('data/vinay.wav')\n",
    "                print(examples_batch.shape)\n",
    "\n",
    "                # Prepare a postprocessor to munge the model embeddings.\n",
    "                pproc = vggish_postprocess.Postprocessor('vggish_pca_params.npz')\n",
    "                \n",
    "               \n",
    "    # Run inference and postprocessing.\n",
    "                [embedding_batch] = sess1.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "                print(embedding_batch.shape)\n",
    "                postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "                print(postprocessed_batch.shape)\n",
    "    \n",
    "              \n",
    "              # train single example\n",
    "                _ , c,l = sess.run([train_op, cost, Logits], feed_dict={X: postprocessed_batch,Y: Next_element[1] })\n",
    "            \n",
    "                 \n",
    "                print(l.shape)\n",
    "                print(c)\n",
    "                #print(pre)\n",
    "      #          print(p)\n",
    "               \n",
    "               \n",
    "              \n",
    "          \n",
    "   \n",
    "        \n",
    "                \n",
    "    # Save model weights to disk\n",
    "   # save_path = saver.save(sess, model_path)\n",
    "    #print(\"Model saved in file: %s\" % save_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/datav2/20_48_00018A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_48_00006A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_54_21022B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "127457.77\n",
      "data/datav2/20_54_21019B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_48_00016A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "380674.84\n",
      "data/datav2/20_50_46056A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "40322.95\n",
      "data/datav2/20_54_21026B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_50_46071A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "2672.7395\n",
      "data/datav2/20_50_46006A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_50_46091A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_50_46014A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_54_21021B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "53010.8\n",
      "data/datav2/20_50_46017A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_54_21004B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "489067.75\n",
      "data/datav2/20_50_46116A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_54_21023B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_54_21001B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "258397.3\n",
      "data/datav2/20_48_00017A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "110723.35\n",
      "data/datav2/20_54_21047B.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[1 1 1]\n",
      "(3, 4)\n",
      "0.0\n",
      "data/datav2/20_50_46041A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "150673.77\n",
      "data/datav2/20_50_46088A.wav\n",
      "(3, 96, 64)\n",
      "(3, 128)\n",
      "(3, 128)\n",
      "[0 0 0]\n",
      "(3, 4)\n",
      "63008.113\n",
      "data/datav2/20_48_00028A.wav\n",
      "(2, 96, 64)\n",
      "(2, 128)\n",
      "(2, 128)\n",
      "[0 0 0]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must have the same first dimension, got logits shape [2,4] and labels shape [3]\n\t [[Node: SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Print, _arg_Placeholder_1_0_1)]]\n\nCaused by op 'SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/home/saurabh/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saurabh/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-e476081f151b>\", line 12, in <module>\n    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=Logits))\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1964, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4804, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2,4] and labels shape [3]\n\t [[Node: SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Print, _arg_Placeholder_1_0_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [2,4] and labels shape [3]\n\t [[Node: SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Print, _arg_Placeholder_1_0_1)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-58436fd79730>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m               \u001b[0;31m# train single example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0m_\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLogits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpostprocessed_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my_temp\u001b[0m \u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1362\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1363\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1365\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must have the same first dimension, got logits shape [2,4] and labels shape [3]\n\t [[Node: SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Print, _arg_Placeholder_1_0_1)]]\n\nCaused by op 'SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits', defined at:\n  File \"/home/saurabh/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/home/saurabh/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n    handler_func(fd_obj, events)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n    user_expressions, allow_stdin)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-9-e476081f151b>\", line 12, in <module>\n    cost = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=Y, logits=Logits))\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\", line 1964, in sparse_softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 4804, in _sparse_softmax_cross_entropy_with_logits\n    labels=labels, name=name)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 787, in _apply_op_helper\n    op_def=op_def)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 3160, in create_op\n    op_def=op_def)\n  File \"/home/saurabh/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1625, in __init__\n    self._traceback = self._graph._extract_stack()  # pylint: disable=protected-access\n\nInvalidArgumentError (see above for traceback): logits and labels must have the same first dimension, got logits shape [2,4] and labels shape [3]\n\t [[Node: SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits = SparseSoftmaxCrossEntropyWithLogits[T=DT_FLOAT, Tlabels=DT_INT32, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"](Print, _arg_Placeholder_1_0_1)]]\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(num_steps):\n",
    "        avg_cost = 0.\n",
    "        \n",
    "        \n",
    "        sess.run(iterator.initializer)\n",
    "        while True:\n",
    "            try:\n",
    "                Next_element=sess.run(next_element)\n",
    "                input_wav_file_name=str(Next_element[0][0],'utf-8')\n",
    "                print(input_wav_file_name)\n",
    "                \n",
    "                examples_batch = vggish_input.wavfile_to_examples(input_wav_file_name)\n",
    "                print(examples_batch.shape)\n",
    "\n",
    "                # Prepare a postprocessor to munge the model embeddings.\n",
    "                pproc = vggish_postprocess.Postprocessor('vggish_pca_params.npz')\n",
    "                \n",
    "               \n",
    "    # Run inference and postprocessing.\n",
    "                [embedding_batch] = sess1.run([embedding_tensor],\n",
    "                                 feed_dict={features_tensor: examples_batch})\n",
    "                print(embedding_batch.shape)\n",
    "                postprocessed_batch = pproc.postprocess(embedding_batch)\n",
    "                print(postprocessed_batch.shape)\n",
    "                  \n",
    "                #y_temp=np.zeros(3)\n",
    "                    \n",
    "                y_temp=np.append( Next_element[1], [ Next_element[1] ,Next_element[1] ]  )\n",
    "                \n",
    "                print(y_temp)\n",
    "\n",
    "              # train single example\n",
    "                _ , c,l = sess.run([train_op, cost, Logits], feed_dict={X: postprocessed_batch,Y: y_temp })\n",
    "            \n",
    "                 \n",
    "                print(l.shape)\n",
    "                print(c)\n",
    "                #print(pre)\n",
    "      #          print(p)\n",
    "               \n",
    "               \n",
    "              \n",
    "            except tf.errors.OutOfRangeError:\n",
    "              break\n",
    "   \n",
    "        \n",
    "                \n",
    "    # Save model weights to disk\n",
    "   # save_path = saver.save(sess, model_path)\n",
    "    #print(\"Model saved in file: %s\" % save_path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, num_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: mnist.test.images,\n",
    "                                      Y: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
